---
title: "Mixture-of-Experts Models for Claim Frequency and Severity"
subtitle: "CAS RPM, Mar 15 2023, San Diego CA"
author:
    - name: "Ian Weng (Sophia) Chan"
      affiliation: "PhD Candidate, University of Toronto"
bibliography: references.bib
format:
  revealjs: 
    theme: white
    toc: true
    toc-depth: 1
    slide-level: 2
    slide-number: true
    margin: 0.1
    center: false
    fig-cap-location: "bottom"
    fig-align: "center"
    chalkboard: 
      buttons: false
    preview-links: auto
    # logo: images/quarto.png
    css: styles.css
    # footer: "CAS RPM, Mar 15 2023, San Diego CA"
# resources:
#   - demo.pdf
---

## Overview

::: incremental
- We propose a flexible Mixture-of-Experts (MoE) framework for modelling claim frequency and severity, and for many other applications

- We showcase a few case studies to demonstrate the theoretical flexibility and wide applicability of the framework

- Our model has been implemented in Julia and R as open-source packages, readily available for a variety of applications

- Joint work by the [actuarial research group](https://actsci.utstat.utoronto.ca/) at the University of Toronto: Tsz Chai (Samson) Fung, Spark Tseung, Prof. Andrei Badescu, and Prof. X. Sheldon Lin
:::

# Motivating Examples

## Actuaries: GLM is great!

::: {.columns}
::: {.column width="60%"}
::: incremental
- GLM is simple yet powerful.

- GLM is easy to implement.

- GLM is interpretable and accessible.

- **However, GLM can fail miserably in insurance applications, because real data do not satisfy GLM assumptions.**
:::
:::

::: {.column width="5%"}
:::

::: {.column width="35%"}
![](plots/glm-book.jpg){width=400 fig-align="left"}
:::

:::

---

## GLM fails when...

::: nonincremental
- Claim frequency distribution is heavy-tailed.

- There is an excess probability of zero claims.
::: 

. . .

Example: Australian auto insurance data (*ausprivauto040*) in *CASDatasets* [@dutang2020package],
analyzed in [@badescu2021actuaryarticle].

![](plots/AUS-hist-log-frequency-ex-MoE.png){width=500 fig-align="center"}

---

## GLM fails when...

::: nonincremental
- Claim severity distribution is multimodal and/or heavy-tailed.

- Observations are censored and/or truncated.
:::

. . .

::: {.columns}
::: {.column width="40%"}
Example: French auto insurance data (*freMTPLsev*) in *CASDatasets* [@dutang2020package],
analyzed in [@tseung2021lrmoe].
:::

::: {.column width="60%"}
![](plots/FRA-hist-log-claim.png){width=450 fig-align="center"}
:::
:::

---

## Insurance data are heterogeneous.

::: nonincremental
- Policyholders' risk profiles are different, even within the same portfolio.
- One way to capture such heterogeneity is to use a **mixture** model.
:::

. . .

Example: Modelling claim frequency with a 3-component Poisson mixture.

![](plots/model-structure-no-regression.png){width=800 fig-align="center"}

---

## Covariates are important.

::: nonincremental
- Policyholders' information, or covariates, are predictive of their risk profile.
- We may use **regression** to classify policyholders into different risk groups, and model each group separately.
:::

. . .

Example: A Poisson mixture model combined with logistic regression.

![](plots/model-structure.png){width=800 fig-align="center"}

---

## MoE = Regression + Mixture

This is an example of our proposed Mixture-of-Experts (MoE) framework.

::: incremental
- We first classify policyholders into different latent risk groups with a multiple logistic regression.
- Within each risk group, we model the response (frequency or severity) with an appropriate distribution (or expert).
:::

![](plots/model-structure.png){width=800 fig-align="center"}

---

## MoE: Flexibile and Powerful

::: nonincremental
- The MoE framework is extremely flexible and powerful (more details later).
- For example, it offers a much better fit to data compared with GLM.
:::

. . .

Example: Australian auto insurance data (*ausprivauto040*) in *CASDatasets* [@dutang2020package],
analyzed in [@badescu2021actuaryarticle].

![](plots/AUS-hist-log-frequency.png){width=500 fig-align="center"}

---

## MoE: Flexibile and Powerful

Example: French auto insurance data (*freMTPLsev*) in *CASDatasets* [@dutang2020package],
analyzed in [@tseung2021lrmoe].

::: {.columns}
::: {.column width="50%"}
![](plots/FRA-hist-vs-fitted.png){width=450 fig-align="center"}
:::

::: {.column width="50%"}
![](plots/FRA-qq-fitted.png){width=450 fig-align="center"}
:::
:::

# A Crash Course on MoE

## Model Setup

::: incremental
- Let $\mathbf{x}_{i} = (x_{i0}, x_{i1}, \dots, x_{iP})^{T}$ denote the $(P+1)$-dimensional covariate vector 
for policyholder $i$ for $i=1, 2, \dots, n$.

- Based on the covariates, policyholder $i$ is classified into one of $g$ latent risk classes by a logit **gating function**
$$
    \pi_{j}(\mathbf{x}_{i}; \mathbf{\alpha}_{j}) = \frac{\exp(\mathbf{\alpha}^{T}_{j}\mathbf{x}_{i})}{\sum_{j^{\prime}=1}^{g} \exp(\mathbf{\alpha}^{T}_{j^{\prime}}\mathbf{x}_{i}) }, \quad j = 1, 2, \dots, g,
$$
where $\mathbf{\alpha}_{j} = (\alpha_{j0}, \alpha_{j1}, \dots, \alpha_{jP})^{T}$ is a vector of regression coefficients for latent class $j$.

- Given the assignment of latent class $j \in \{ 1, 2, \dots, g \}$, the response variables $\mathbf{y}_{i}$ are modelled by an
**expert function** $f_j(\mathbf{y}_{i}; \mathbf{\varphi}_j)$ with parameters $\mathbf{\varphi}_j$.
:::

## Example: 3-Component MoE

![](plots/moe-model-setup.png){width=450 fig-align="center"}

## More Details
### Miltiple Logit for Classification

::: incremental
- We have assumed a multiple logistic gating function for classification into latent risk classes.
$$
    \pi_{j}(\mathbf{x}_{i}; \mathbf{\alpha}_{j}) = \frac{\exp(\mathbf{\alpha}^{T}_{j}\mathbf{x}_{i})}{\sum_{j^{\prime}=1}^{g} \exp(\mathbf{\alpha}^{T}_{j^{\prime}}\mathbf{x}_{i}) }, \quad j = 1, 2, \dots, g,
$$

- It is possible to pose other functions for this classification stage, e.g. a probit regression.

- The multiple logistic regression is a good choice for its interpretability and ease of computational implementation.
:::

## More Details
### Expert Functions for the Response

::: incremental
- In the expert functions $f_j(\mathbf{y}_{i}; \mathbf{\varphi}_j)$, we have assumed some **fixed** parameters
$\mathbf{\varphi}_j$ which are **independent** of the covariates $\mathbf{x}_{i}$.

- It is possible to also incorporate the covariates $\mathbf{x}_{i}$ in the parameters of the expert functions such that
$$
    f_j(\mathbf{y}_{i}; \mathbf{\varphi}_j(\mathbf{x}_{i})) = f_j(\mathbf{y}_{i}; \mathbf{\varphi}_j(\mathbf{x}_{i}, \mathbf{\beta}_j))
$$
where $\mathbf{\beta}_j$ is a vector of regression coefficients for $j$-th expert function.

- The interpretation of our framework: the risk profiles of policyholders within the same latent group
are homogeneous and independent of their covariates.
:::

## LRMoE

::: incremental
- Our modelling framework is called the **Logit-weighted Reduced** Mixture-of-Experts (LRMoE) due to
the simplifying assumptions on the expert functions.

- It is a special case of the more general class of Mixture-of-Experts (MoE) models, see e.g. [@jordan1994hierarchical]
(figure extracted from the paper).

  ![](plots/jordan-general-moe-setup.png){width=500 fig-align="center"}
:::

## Simpler is sometimes better

### ... or at least equally good.

::: incremental
- The general modelling framework of MoE is extremely flexible and powerful.

- However, our LRMoE framework has a much **simpler** structure and 
it can be **as good as** the general MoE in terms of fitting the data.

- Indeed, actuaries may sometimes prefer models with a simpler structure.
:::


## More Details

### Dependence of Response Variables

::: incremental
- The response variables $\mathbf{y}_{i}$ may be multi-dimensional, e.g., $\mathbf{y}_{i} = (y_{i1}, y_{i2})$ 
for two auto insurance coverages purchased by the same policyholder.
- In this case, we assume **conditional independence** between the two dimensions of $\mathbf{y}_{i}$,
assignment of latent class $j$.
- However, they are in fact **dependent** due to the mixture structure, which is essential for capturing
the **correlation** between the two coverages observed in real data.
:::

# Appendix

## References

::: {#refs}
:::